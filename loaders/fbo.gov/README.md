# Importing FedBizOpps data into FBOpen using the fbo.gov loader

## Limitations
* The fbo.gov loader currently loads only **COMBINE**, **PRESOL** and **MOD** opportunity notices.
* **MOD handling is limited.** Current behavior is to fully update the existing record with a MOD, while appending the new description to the old.
* The attachment crawler/loader is still very primitive. It works for attachments uploaded to fbo.gov or to which the listing's synopsis links directly. Attachments that are more than one hop removed from the listing page are *not* retrieved, but we hope to improve the crawler to do this.

## To do
* load AWARD and other notices types
* survey FBO listing archetypes to determine the algorithms for retrieving the relevant attachments for various types of notices, and implement that logic in the crawler/loader
* better failure handling for the metadata loader, attachment crawler, and attachment loader
* script to automatically run the nightly update

## To install the loader
* Install Elasticsearch. On OS X, this is as easy as `brew install elasticsearch`.
* Create an index on your Elasticsearch cluster:
    * `curl -XPUT localhost:9200/fbopen`
* In the loaders/fbo.gov/ directory:
	* `sudo npm install`
        * (note: `sudo` is required so that the json package can be installed globally.)

## To import FedBizOpps data
After install, first load a full set of data using the one-time/weekly loader. Then you can simply run nightly updates.

### 1. One-time/Weekly full load
1. **`fbo-weekly.sh`**

	Downloads the full, weekly FBO XML file from ftp://ftp.fbo.gov.
	From the fbo.gov weekly XML data dump, creates and loads a set of Elasticsearch bulk load files.
	
	Optional ENV vars:
	
	`$FBO_WEEKLY_XML_FILE` = where to save the downloaded file; default = *./workfiles/FBOFullXML.xml*
	`$ELASTICSEARCH_URI` = the URI of your Elasticsearch instance; default = *localhost:9200*
	`$ELASTICSEARCH_INDEX` = the name of the ES index to load into; default = *fbopen*
	
	Optional arguments:

 	`$1` = output filepath/name of the list of links to the FBO listings that were prepped; default = *./workfiles/listings-links.txt*

 	`$2` = directory into which attachments should be downloaded; default = *./fbo-attachments*


4. **`process-listing-links.sh < workfiles/listings-links.txt`** *(or alternate filename of list of links generated by step 2)*

	Downloads and loads into Solr the relevant attachments from each URL in the input stream. Assumes those URLs are fbo.gov listing pages.
	
	**NOTE 1:** This process is currently very slow, as it's completely synchronous. This is intentional; trying to download too many files at once, and trying to load too many into Solr at once, led to problems. Better would be (1) a throttled async version that handles several file downloads simultaneously, and (2) Do the Solr ingestion in a completely separate step.
	
	**NOTE 2:** It's also very buggy. There are many files that shouldn't be downloaded but are, and there are many files that are behind multiple clicks and/or unreachable without login. The algorithm for doing these downloads will have to improve significantly.
	
### Nightly updates
All at once: **`fbo-nightly.sh [YYYYMMDD]`** (defaults to yesterday)

Or, if you want to do it in steps:

* To download the nightly data file: `node nightly-fbo-parser.js -o [-d YYYYMMDD]` (defaults to yesterday)
* To load the nightly data into FBOpen: `node nightly-fbo-parser.js [-d YYYYMMDD]` (defaults to yesterday)
* To collect and load listings' attachments into FBOpen: `process-listing-links.sh < links-YYYYMMDD.txt`

The parser in the nightly code -- that is, the hard part -- is borrowed wholesale from our predecessor [Adam Becker's](https://github.com/adamjacobbecker/) [fbo-parser](https://github.com/presidential-innovation-fellows/fbo-parser). Thanks Adam!
